<html>
<head>
<title>Fine Tuning pretrained BERT for Sentiment Classification using Transformers in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨ Python ä¸­çš„è½¬æ¢å™¨å¾®è°ƒé¢„è®­ç»ƒçš„ç”¨äºæƒ…æ„Ÿåˆ†ç±»çš„ BERT</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/nerd-for-tech/fine-tuning-pretrained-bert-for-sentiment-classification-using-transformers-in-python-931ed142e37?source=collection_archive---------2-----------------------#2021-08-08">https://medium.com/nerd-for-tech/fine-tuning-pretrained-bert-for-sentiment-classification-using-transformers-in-python-931ed142e37?source=collection_archive---------2-----------------------#2021-08-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/b6933a073a37931f6d59d2aef157a725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*eWmEKPSrZIlLTU-LvApTYw.png"/></div></figure><h1 id="7283" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æƒ…æ„Ÿåˆ†æ</h1><p id="5502" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">æƒ…æ„Ÿåˆ†ææ˜¯è‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„ä¸€ä¸ªåº”ç”¨ï¼Œç”¨äºå‘ç°ç”¨æˆ·è¯„è®ºã€è¯„è®ºç­‰çš„æƒ…æ„Ÿã€‚åœ¨ç½‘ä¸Šã€‚å¦‚ä»Šï¼Œåƒè„¸ä¹¦ã€æ¨ç‰¹è¿™æ ·çš„ç¤¾äº¤ç½‘ç«™è¢«å¹¿æ³›ç”¨äºå‘å¸ƒç”¨æˆ·å¯¹ä¸åŒäº‹ç‰©çš„è¯„è®ºï¼Œæ¯”å¦‚ç”µå½±ã€æ–°é—»ã€ç¾é£Ÿã€æ—¶å°šã€æ”¿æ²»ç­‰ç­‰ã€‚è¯„è®ºå’Œæ„è§åœ¨ç¡®å®šç”¨æˆ·å¯¹ç‰¹å®šå®ä½“çš„æ»¡æ„åº¦æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶åï¼Œè¿™äº›ç”¨äºæ‰¾åˆ°ææ€§ï¼Œå³æ­£æã€è´Ÿæå’Œä¸­æ€§ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œè®¨è®ºäº†ä¸€ç§å¯¹ç”µå½±è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æçš„æ–¹æ³•ã€‚</p><h1 id="f74b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">é—®é¢˜é™ˆè¿°</h1><p id="66d4" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">ABC å…¬å¸å¸Œæœ›æ‹¥æœ‰ä¸€ä¸ªé«˜åº¦å¯æ‰©å±•çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚å› æ­¤ï¼Œå®ƒå†³å®šè¶…è¶Šä»–ä»¬çš„è¯„è®ºç”Ÿæ€ç³»ç»Ÿï¼Œå¹¶æ ¹æ®æ›´å¤šæ•°æ®è®­ç»ƒä»–ä»¬ç°æœ‰çš„æ¨¡å‹ã€‚è®­ç»ƒæ ·æœ¬æ˜¯è¯„è®ºå’Œæ¨æ–‡çš„æ··åˆã€‚</p><h1 id="4748" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å±æ€§æè¿°:</h1><ul class=""><li id="a290" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">ID â€”å”¯ä¸€æ ‡è¯†ç¬¦</li><li id="4998" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">ä½œè€…-ç”¨æˆ· ID</li><li id="79f0" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">å®¡æŸ¥â€”ä¸åŒç±»å‹</li><li id="9ddf" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">ç±»åˆ«â€”ä»£è¡¨å„ç§æƒ…ç»ª(<strong class="jm hj"> 0 </strong>:è´Ÿé¢ï¼Œ<strong class="jm hj"> 1 </strong>:ä¸­æ€§<strong class="jm hj"> 2 </strong>:æ­£é¢)</li></ul><h1 id="02ed" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å¿…éœ€çš„å®‰è£…</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/57da2930027686cc196d578f335b1af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*Kig1wSN3RbzOxAcTUBpx4g.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated"><strong class="bd io">æŠ±è„¸å˜å‹å™¨</strong></figcaption></figure><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="bbe8" class="lk in hi lg b fi ll lm l ln lo">!pip install transformers</span></pre><h1 id="dc22" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å¯¼å…¥æ‰€éœ€çš„åŒ…</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="e988" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">import</strong> <strong class="lg hj">pandas</strong> <strong class="lg hj">as</strong> <strong class="lg hj">pd</strong><br/><strong class="lg hj">import</strong> <strong class="lg hj">numpy</strong> <strong class="lg hj">as</strong> <strong class="lg hj">np</strong><br/><strong class="lg hj">import</strong> <strong class="lg hj">seaborn</strong> <strong class="lg hj">as</strong> <strong class="lg hj">sns</strong><br/><strong class="lg hj">import</strong> <strong class="lg hj">matplotlib.pyplot</strong> <strong class="lg hj">as</strong> <strong class="lg hj">plt</strong><br/>%matplotlib inline<br/><strong class="lg hj">import</strong> <strong class="lg hj">warnings</strong><br/>warnings.filterwarnings('ignore')</span></pre><h1 id="89bb" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">è¯»å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ ·æœ¬</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="205a" class="lk in hi lg b fi ll lm l ln lo">train = pd.read_csv("/content/disk/MyDrive/Machine_Hack/train.csv")<br/>test = pd.read_csv("/content/disk/MyDrive/Machine_Hack/test.csv")<br/>sub = pd.read_csv("/content/disk/MyDrive/Machine_Hack/submission.csv")</span><span id="552c" class="lk in hi lg b fi lp lm l ln lo">train.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/d5d2df29a7b22cedb821866c3c557d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kw6Wf6TNsjlqtaUqblg6xw.png"/></div></div></figure><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="4f22" class="lk in hi lg b fi ll lm l ln lo">test.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/d7ad594821038c45a2592b953460084f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*mXE_elWQhibFTRGwsgQfFQ.png"/></div></figure><h1 id="43e0" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">è®­ç»ƒæ ·æœ¬ä¸­æ ‡ç­¾çš„åˆ†å¸ƒ</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/4c47c3ee49866e98b3749ea5c42bd5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*nvzF7NNaDqSdA0STIRntLA.png"/></div></figure><h1 id="3a84" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æå–è¦ç´ å’Œæ ‡æ³¨</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="caab" class="lk in hi lg b fi ll lm l ln lo">train_texts = train['Review'].values.tolist()<br/>train_labels = train['Sentiment'].values.tolist()<br/>test_texts = test['Review'].values.tolist()</span></pre><h1 id="acf6" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å°†è®­ç»ƒæ ·æœ¬åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="8404" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">from</strong> <strong class="lg hj">sklearn.model_selection</strong> <strong class="lg hj">import</strong> train_test_split<br/>train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2,random_state=42,stratify=train_labels)</span></pre><h1 id="31bd" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å¾®è°ƒå®šåˆ¶æ¨¡å‹çš„æ­¥éª¤</h1><ul class=""><li id="661c" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">å‡†å¤‡æ•°æ®é›†</li><li id="47b1" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">åŠ è½½é¢„è®­ç»ƒçš„æ ‡è®°å™¨ï¼Œç”¨æ•°æ®é›†è°ƒç”¨å®ƒ</li><li id="d654" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">ä½¿ç”¨ç¼–ç æ„å»º Pytorch æ•°æ®é›†</li><li id="c505" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">è´Ÿè½½é¢„è®­ç»ƒæ¨¡å‹</li><li id="517f" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">åŠ è½½è®­ç»ƒå™¨å¹¶è®­ç»ƒå®ƒ(æˆ–)ä½¿ç”¨æœ¬æœº Pytorch è®­ç»ƒç®¡é“</li></ul><p id="f7e1" class="pw-post-body-paragraph jk jl hi jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh hb bi translated">æ³¨æ„:è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†åŸ¹è®­å¸ˆ</p><h1 id="49ca" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å¯¼å…¥æ‰€éœ€çš„è½¬æ¢å™¨åº“</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="8b02" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">import</strong> <strong class="lg hj">torch</strong><br/><strong class="lg hj">from</strong> <strong class="lg hj">torch.utils.data</strong> <strong class="lg hj">import</strong> Dataset<br/><strong class="lg hj">from</strong> <strong class="lg hj">transformers</strong> <strong class="lg hj">import</strong> DistilBertTokenizerFast,DistilBertForSequenceClassification<br/><strong class="lg hj">from</strong> <strong class="lg hj">transformers</strong> <strong class="lg hj">import</strong> Trainer,TrainingArguments</span></pre><h1 id="5bdd" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">è®¾ç½®æ¨¡å‹åç§°</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="82fc" class="lk in hi lg b fi ll lm l ln lo">model_name  = 'distilbert-base-uncased'</span></pre><h1 id="6cb2" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æ ‡è®°åŒ–</h1><p id="89b8" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">ä½¿ç”¨ tokenizer å¯¹è¯­æ–™åº“è¿›è¡Œç¼–ç ã€‚</p><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="419b" class="lk in hi lg b fi ll lm l ln lo">tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',num_labels=3)</span></pre><ul class=""><li id="00d6" class="ki kj hi jm b jn lx jr ly jv mc jz md kd me kh kn ko kp kq bi translated">è¿™é‡Œæ ‡ç­¾çš„æ•°é‡=3</li></ul><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="aa6a" class="lk in hi lg b fi ll lm l ln lo">train_encodings = tokenizer(train_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = 'pt')<br/>val_encodings = tokenizer(val_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = 'pt')<br/>test_encodings = tokenizer(test_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = 'pt')</span></pre><ul class=""><li id="1377" class="ki kj hi jm b jn lx jr ly jv mc jz md kd me kh kn ko kp kq bi translated">åœ¨ BERT æƒ…å†µä¸‹ï¼Œè®¾ç½® truncation = True å°†æ¶ˆé™¤è¶…è¿‡ max_length(512)çš„ä»¤ç‰Œ</li><li id="8853" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">è®¾ç½® padding =True å°†ä½¿ç”¨ç©ºæ ‡è®°(å³ 0)å¡«å……é•¿åº¦å°äº max_length çš„æ–‡æ¡£ï¼Œç¡®ä¿æˆ‘ä»¬çš„æ‰€æœ‰åºåˆ—éƒ½å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦</li><li id="2418" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">è®¾ç½® return_tensors = 'pt 'ä¼šå°†ç¼–ç è¿”å›ä¸º pytorch å¼ é‡</li><li id="e2dd" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">è¿™å°†å…è®¸æˆ‘ä»¬åŒæ—¶å‘æ¨¡å‹ä¸­è¾“å…¥ä¸€æ‰¹åºåˆ—ã€‚</li></ul><h1 id="1703" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å°†æˆ‘ä»¬çš„æ ‡ç­¾å’Œç¼–ç è½¬æ¢æˆæ•°æ®é›†å¯¹è±¡</h1><ul class=""><li id="16e9" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">å°†æ ‡è®°åŒ–çš„æ•°æ®åŒ…è£…åˆ° torch æ•°æ®é›†ä¸­</li><li id="4ffd" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">åœ¨ PyTorch ä¸­ï¼Œè¿™æ˜¯é€šè¿‡å­ç±»åŒ– torch.utils.data.Dataset å¯¹è±¡å¹¶å®ç° len å’Œ getitem æ¥å®ç°çš„ã€‚</li></ul><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="041d" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">class</strong> <strong class="lg hj">SentimentDataset</strong>(torch.utils.data.Dataset):<br/>    <strong class="lg hj">def</strong> __init__(self, encodings, labels):<br/>        self.encodings = encodings<br/>        self.labels = labels<br/><br/>    <strong class="lg hj">def</strong> __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) <strong class="lg hj">for</strong> key, val <strong class="lg hj">in</strong> self.encodings.items()}<br/>        item['labels'] = torch.tensor(self.labels[idx])<br/>        <strong class="lg hj">return</strong> item<br/><br/>    <strong class="lg hj">def</strong> __len__(self):<br/>        <strong class="lg hj">return</strong> len(self.labels)<br/>## Test Dataset</span><span id="0374" class="lk in hi lg b fi lp lm l ln lo"><strong class="lg hj">class</strong> <strong class="lg hj">SentimentTestDataset</strong>(torch.utils.data.Dataset):<br/>    <strong class="lg hj">def</strong> __init__(self, encodings):<br/>        self.encodings = encodings<br/><br/>    <strong class="lg hj">def</strong> __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) <strong class="lg hj">for</strong> key, val <strong class="lg hj">in</strong> self.encodings.items()}<br/>        <strong class="lg hj">return</strong> item<br/>    <strong class="lg hj">def</strong> __len__(self):<br/>        <strong class="lg hj">return</strong> len(self.encodings)</span></pre><h1 id="ab4a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">Genearte æ•°æ®åŠ è½½å™¨</h1><ul class=""><li id="6f1a" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">å°†æ ‡è®°åŒ–çš„æ•°æ®è½¬æ¢ä¸º torch æ•°æ®é›†</li></ul><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="73f3" class="lk in hi lg b fi ll lm l ln lo">train_dataset = SentimentDataset(train_encodings, train_labels)<br/>val_dataset = SentimentDataset(val_encodings, val_labels)<br/>test_dataset = SentimentTestDataset(test_encodings)</span></pre><h1 id="ce4a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å®šä¹‰ä¸€ä¸ªç®€å•çš„åº¦é‡å‡½æ•°</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="df2b" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">from</strong> <strong class="lg hj">sklearn.metrics</strong> <strong class="lg hj">import</strong> accuracy_score, f1_score<br/><strong class="lg hj">def</strong> compute_metrics(p):<br/>    pred, labels = p<br/>    pred = np.argmax(pred, axis=1)<br/><br/>    accuracy = accuracy_score(y_true=labels, y_pred=pred)<br/>    <em class="mf">#recall = recall_score(y_true=labels, y_pred=pred)</em><br/>    <em class="mf">#precision = precision_score(y_true=labels, y_pred=pred)</em><br/>    f1 = f1_score(labels, pred, average='weighted')<br/><br/>    <strong class="lg hj">return</strong> {"accuracy": accuracy,"f1_score":f1}</span></pre><h1 id="e45a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å®šä¹‰åŸ¹è®­å‚æ•°</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="ce7f" class="lk in hi lg b fi ll lm l ln lo">training_args = TrainingArguments(<br/>    output_dir='./res',          <em class="mf"># output directory</em><br/>    evaluation_strategy="steps",<br/>    num_train_epochs=5,              <em class="mf"># total number of training epochs</em><br/>    per_device_train_batch_size=32,  <em class="mf"># batch size per device during training</em><br/>    per_device_eval_batch_size=64,   <em class="mf"># batch size for evaluation</em><br/>    warmup_steps=500,                <em class="mf"># number of warmup steps for learning rate scheduler</em><br/>    weight_decay=0.01,               <em class="mf"># strength of weight decay</em><br/>    logging_dir='./logs4',            <em class="mf"># directory for storing logs</em><br/>    <em class="mf">#logging_steps=10,</em><br/>    load_best_model_at_end=<strong class="lg hj">True</strong>,<br/>)</span></pre><h1 id="7941" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">ä½¿ç”¨æ•™ç»ƒè¿›è¡Œå¾®è°ƒ</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="6e2c" class="lk in hi lg b fi ll lm l ln lo">model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",num_labels=3)<br/><br/>trainer = Trainer(<br/>    model=model,<em class="mf"># the instantiated ğŸ¤— Transformers model to be trained</em><br/>    args=training_args, <em class="mf"># training arguments, defined above</em><br/>    train_dataset=train_dataset,<em class="mf"># training dataset</em><br/>    eval_dataset=val_dataset , <em class="mf"># evaluation dataset</em><br/>    compute_metrics=compute_metrics,<br/>)<br/><br/>trainer.train()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/ec4ca08e4981ee6ae5ddeb338e1d714f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*N0di1LimmQR83zHSiCHVcQ.png"/></div></figure><h1 id="7a8b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">åŸºäºéªŒè¯æ•°æ®é›†è¯„ä¼°åŸ¹è®­å¸ˆ</h1><p id="75b6" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">å› ä¸ºä½œä¸ºè®­ç»ƒçš„ä¸€éƒ¨åˆ†ï¼Œload_best_model_at_end è¢«è®¾ç½®ä¸º Trueï¼Œæ‰€ä»¥è¿™å°†åœ¨å®Œæˆè®­ç»ƒæ—¶è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ã€‚</p><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="c779" class="lk in hi lg b fi ll lm l ln lo">trainer.evaluate()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/dd32e4b33160ebbd791a30f6d305e0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2HZxmgus6PrMkbOw58ZgjA.png"/></div></figure><h1 id="175b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æ ¹æ®æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="dded" class="lk in hi lg b fi ll lm l ln lo">test[â€˜Sentimentâ€™] = 0 test_texts = test[â€˜Reviewâ€™].values.tolist() test_labels = test[â€˜Sentimentâ€™].values.tolist() <br/>test_encodings = tokenizer(test_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = â€˜ptâ€™).to(â€œcudaâ€) <br/>test_dataset = SentimentDataset(test_encodings, test_labels)</span><span id="ee81" class="lk in hi lg b fi lp lm l ln lo">preds = trainer.predict(test_dataset=test_dataset)</span></pre><h1 id="7ff7" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æ£€ç´¢é¢„æµ‹æ¦‚ç‡</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="caff" class="lk in hi lg b fi ll lm l ln lo">probs = torch.from_numpy(preds[0]).softmax(1)<br/><br/>predictions = probs.numpy()# convert tensors to numpy array</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mi"><img src="../Images/b5b5059bfaf504757da724bfcc4f28c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*SsuoQdBmK8PPuth0tOWp6w.png"/></div></div></figure><h1 id="ced4" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å°†ç›¸å…³çš„é¢„æµ‹æ¦‚ç‡è½¬æ¢æˆæ•°æ®å¸§</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="3bfa" class="lk in hi lg b fi ll lm l ln lo">newdf = pd.DataFrame(predictions,columns=['Negative_0','Neutral_1','Positive_2'])<br/>new_df.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/f39bcdd224c930b56e1a934f26c3e652.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*1AN3RRIh0pPSPuhV7iSF0g.png"/></div></figure><h1 id="421f" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">å®šä¹‰å‡½æ•°ä»¥æ ¼å¼åŒ–é¢„æµ‹æ ‡ç­¾</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="dde9" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">def</strong> labels(x):<br/>  <strong class="lg hj">if</strong> x == 0:<br/>    <strong class="lg hj">return</strong> 'Negative_0'<br/>  <strong class="lg hj">elif</strong> x == 1:<br/>    <strong class="lg hj">return</strong> 'Neutral_1'<br/>  <strong class="lg hj">else</strong>:<br/>    <strong class="lg hj">return</strong> 'Positive_2'<br/><br/>results = np.argmax(predictions,axis=1)<br/>test['Sentiment'] = results<br/>test['Sentiment'] = test['Sentiment'].map(labels)<br/>test.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mk"><img src="../Images/b92fe0979801d8754e6ee352df42382a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43vJsctUofBSnrSR2ovDgw.png"/></div></div></figure><h1 id="4617" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æƒ³è±¡ä¸€ä¸‹é¢„æµ‹</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="6500" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">import</strong> <strong class="lg hj">seaborn</strong> <strong class="lg hj">as</strong> <strong class="lg hj">sns</strong><br/>sns.countplot(x='Sentiment',data=test)</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/ae1c89719f8204887a85e357872e75a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*c4n82rpzHMSHpgjhsdR3Ew.png"/></div></figure><h1 id="7105" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">ç»“è®º</h1><ul class=""><li id="4b73" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šä½¿ç”¨ huggingface transformers åº“è®­ç»ƒäº† BERT æ¨¡å‹</li><li id="572c" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">åŒæ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–å‹å·çš„å˜å‹å™¨ï¼Œå¦‚å¸¦æœ‰ GPT 2 çš„ GPT-2 å˜å‹å™¨ã€å¸¦æœ‰ distilt for sequence classification çš„ DistilBert å˜å‹å™¨ç­‰ã€‚</li></ul><h1 id="9649" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">æ¨èäºº:</h1><div class="mm mn ez fb mo mp"><a href="https://huggingface.co/transformers/training.html" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">PyTorch å’Œ TensorFlow 2.0 çš„æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‚å˜å½¢é‡‘åˆšæä¾›äº†æˆåƒä¸Šä¸‡çš„â€¦</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">huggingface.co</p></div></div><div class="my l"><div class="mz l na nb nc my nd ik mp"/></div></div></a></div><h2 id="0b87" class="lk in hi bd io ne nf ng is nh ni nj iw jv nk nl ja jz nm nn je kd no np ji nq bi translated"><a class="ae nr" href="https://machinehack.com/practice/sentiment_analysis_with_rnn/overview" rel="noopener ugc nofollow" target="_blank">æ•°æ®</a></h2><p id="e5a6" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><a class="ae nr" href="https://www.linkedin.com/in/plaban-nayak-a9433a25/" rel="noopener ugc nofollow" target="_blank">è”ç³»æˆ‘</a></p></div></div>    
</body>
</html>