# ä½¿ç”¨ Python ä¸­çš„è½¬æ¢å™¨å¾®è°ƒé¢„è®­ç»ƒçš„ç”¨äºæƒ…æ„Ÿåˆ†ç±»çš„ BERT

> åŸæ–‡ï¼š<https://medium.com/nerd-for-tech/fine-tuning-pretrained-bert-for-sentiment-classification-using-transformers-in-python-931ed142e37?source=collection_archive---------2----------------------->

![](img/b6933a073a37931f6d59d2aef157a725.png)

# æƒ…æ„Ÿåˆ†æ

æƒ…æ„Ÿåˆ†ææ˜¯è‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„ä¸€ä¸ªåº”ç”¨ï¼Œç”¨äºå‘ç°ç”¨æˆ·è¯„è®ºã€è¯„è®ºç­‰çš„æƒ…æ„Ÿã€‚åœ¨ç½‘ä¸Šã€‚å¦‚ä»Šï¼Œåƒè„¸ä¹¦ã€æ¨ç‰¹è¿™æ ·çš„ç¤¾äº¤ç½‘ç«™è¢«å¹¿æ³›ç”¨äºå‘å¸ƒç”¨æˆ·å¯¹ä¸åŒäº‹ç‰©çš„è¯„è®ºï¼Œæ¯”å¦‚ç”µå½±ã€æ–°é—»ã€ç¾é£Ÿã€æ—¶å°šã€æ”¿æ²»ç­‰ç­‰ã€‚è¯„è®ºå’Œæ„è§åœ¨ç¡®å®šç”¨æˆ·å¯¹ç‰¹å®šå®ä½“çš„æ»¡æ„åº¦æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶åï¼Œè¿™äº›ç”¨äºæ‰¾åˆ°ææ€§ï¼Œå³æ­£æã€è´Ÿæå’Œä¸­æ€§ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œè®¨è®ºäº†ä¸€ç§å¯¹ç”µå½±è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æçš„æ–¹æ³•ã€‚

# é—®é¢˜é™ˆè¿°

ABC å…¬å¸å¸Œæœ›æ‹¥æœ‰ä¸€ä¸ªé«˜åº¦å¯æ‰©å±•çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚å› æ­¤ï¼Œå®ƒå†³å®šè¶…è¶Šä»–ä»¬çš„è¯„è®ºç”Ÿæ€ç³»ç»Ÿï¼Œå¹¶æ ¹æ®æ›´å¤šæ•°æ®è®­ç»ƒä»–ä»¬ç°æœ‰çš„æ¨¡å‹ã€‚è®­ç»ƒæ ·æœ¬æ˜¯è¯„è®ºå’Œæ¨æ–‡çš„æ··åˆã€‚

# å±æ€§æè¿°:

*   ID â€”å”¯ä¸€æ ‡è¯†ç¬¦
*   ä½œè€…-ç”¨æˆ· ID
*   å®¡æŸ¥â€”ä¸åŒç±»å‹
*   ç±»åˆ«â€”ä»£è¡¨å„ç§æƒ…ç»ª( **0** :è´Ÿé¢ï¼Œ **1** :ä¸­æ€§ **2** :æ­£é¢)

# å¿…éœ€çš„å®‰è£…

![](img/57da2930027686cc196d578f335b1af0.png)

**æŠ±è„¸å˜å‹å™¨**

```
!pip install transformers
```

# å¯¼å…¥æ‰€éœ€çš„åŒ…

```
**import** **pandas** **as** **pd**
**import** **numpy** **as** **np**
**import** **seaborn** **as** **sns**
**import** **matplotlib.pyplot** **as** **plt**
%matplotlib inline
**import** **warnings**
warnings.filterwarnings('ignore')
```

# è¯»å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ ·æœ¬

```
train = pd.read_csv("/content/disk/MyDrive/Machine_Hack/train.csv")
test = pd.read_csv("/content/disk/MyDrive/Machine_Hack/test.csv")
sub = pd.read_csv("/content/disk/MyDrive/Machine_Hack/submission.csv")train.head()
```

![](img/d5d2df29a7b22cedb821866c3c557d93.png)

```
test.head()
```

![](img/d7ad594821038c45a2592b953460084f.png)

# è®­ç»ƒæ ·æœ¬ä¸­æ ‡ç­¾çš„åˆ†å¸ƒ

![](img/4c47c3ee49866e98b3749ea5c42bd5ee.png)

# æå–è¦ç´ å’Œæ ‡æ³¨

```
train_texts = train['Review'].values.tolist()
train_labels = train['Sentiment'].values.tolist()
test_texts = test['Review'].values.tolist()
```

# å°†è®­ç»ƒæ ·æœ¬åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†

```
**from** **sklearn.model_selection** **import** train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2,random_state=42,stratify=train_labels)
```

# å¾®è°ƒå®šåˆ¶æ¨¡å‹çš„æ­¥éª¤

*   å‡†å¤‡æ•°æ®é›†
*   åŠ è½½é¢„è®­ç»ƒçš„æ ‡è®°å™¨ï¼Œç”¨æ•°æ®é›†è°ƒç”¨å®ƒ
*   ä½¿ç”¨ç¼–ç æ„å»º Pytorch æ•°æ®é›†
*   è´Ÿè½½é¢„è®­ç»ƒæ¨¡å‹
*   åŠ è½½è®­ç»ƒå™¨å¹¶è®­ç»ƒå®ƒ(æˆ–)ä½¿ç”¨æœ¬æœº Pytorch è®­ç»ƒç®¡é“

æ³¨æ„:è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†åŸ¹è®­å¸ˆ

# å¯¼å…¥æ‰€éœ€çš„è½¬æ¢å™¨åº“

```
**import** **torch**
**from** **torch.utils.data** **import** Dataset
**from** **transformers** **import** DistilBertTokenizerFast,DistilBertForSequenceClassification
**from** **transformers** **import** Trainer,TrainingArguments
```

# è®¾ç½®æ¨¡å‹åç§°

```
model_name  = 'distilbert-base-uncased'
```

# æ ‡è®°åŒ–

ä½¿ç”¨ tokenizer å¯¹è¯­æ–™åº“è¿›è¡Œç¼–ç ã€‚

```
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',num_labels=3)
```

*   è¿™é‡Œæ ‡ç­¾çš„æ•°é‡=3

```
train_encodings = tokenizer(train_texts, truncation=**True**, padding=**True**,return_tensors = 'pt')
val_encodings = tokenizer(val_texts, truncation=**True**, padding=**True**,return_tensors = 'pt')
test_encodings = tokenizer(test_texts, truncation=**True**, padding=**True**,return_tensors = 'pt')
```

*   åœ¨ BERT æƒ…å†µä¸‹ï¼Œè®¾ç½® truncation = True å°†æ¶ˆé™¤è¶…è¿‡ max_length(512)çš„ä»¤ç‰Œ
*   è®¾ç½® padding =True å°†ä½¿ç”¨ç©ºæ ‡è®°(å³ 0)å¡«å……é•¿åº¦å°äº max_length çš„æ–‡æ¡£ï¼Œç¡®ä¿æˆ‘ä»¬çš„æ‰€æœ‰åºåˆ—éƒ½å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦
*   è®¾ç½® return_tensors = 'pt 'ä¼šå°†ç¼–ç è¿”å›ä¸º pytorch å¼ é‡
*   è¿™å°†å…è®¸æˆ‘ä»¬åŒæ—¶å‘æ¨¡å‹ä¸­è¾“å…¥ä¸€æ‰¹åºåˆ—ã€‚

# å°†æˆ‘ä»¬çš„æ ‡ç­¾å’Œç¼–ç è½¬æ¢æˆæ•°æ®é›†å¯¹è±¡

*   å°†æ ‡è®°åŒ–çš„æ•°æ®åŒ…è£…åˆ° torch æ•°æ®é›†ä¸­
*   åœ¨ PyTorch ä¸­ï¼Œè¿™æ˜¯é€šè¿‡å­ç±»åŒ– torch.utils.data.Dataset å¯¹è±¡å¹¶å®ç° len å’Œ getitem æ¥å®ç°çš„ã€‚

```
**class** **SentimentDataset**(torch.utils.data.Dataset):
    **def** __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    **def** __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) **for** key, val **in** self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        **return** item

    **def** __len__(self):
        **return** len(self.labels)
## Test Dataset**class** **SentimentTestDataset**(torch.utils.data.Dataset):
    **def** __init__(self, encodings):
        self.encodings = encodings

    **def** __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) **for** key, val **in** self.encodings.items()}
        **return** item
    **def** __len__(self):
        **return** len(self.encodings)
```

# Genearte æ•°æ®åŠ è½½å™¨

*   å°†æ ‡è®°åŒ–çš„æ•°æ®è½¬æ¢ä¸º torch æ•°æ®é›†

```
train_dataset = SentimentDataset(train_encodings, train_labels)
val_dataset = SentimentDataset(val_encodings, val_labels)
test_dataset = SentimentTestDataset(test_encodings)
```

# å®šä¹‰ä¸€ä¸ªç®€å•çš„åº¦é‡å‡½æ•°

```
**from** **sklearn.metrics** **import** accuracy_score, f1_score
**def** compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)

    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    *#recall = recall_score(y_true=labels, y_pred=pred)*
    *#precision = precision_score(y_true=labels, y_pred=pred)*
    f1 = f1_score(labels, pred, average='weighted')

    **return** {"accuracy": accuracy,"f1_score":f1}
```

# å®šä¹‰åŸ¹è®­å‚æ•°

```
training_args = TrainingArguments(
    output_dir='./res',          *# output directory*
    evaluation_strategy="steps",
    num_train_epochs=5,              *# total number of training epochs*
    per_device_train_batch_size=32,  *# batch size per device during training*
    per_device_eval_batch_size=64,   *# batch size for evaluation*
    warmup_steps=500,                *# number of warmup steps for learning rate scheduler*
    weight_decay=0.01,               *# strength of weight decay*
    logging_dir='./logs4',            *# directory for storing logs*
    *#logging_steps=10,*
    load_best_model_at_end=**True**,
)
```

# ä½¿ç”¨æ•™ç»ƒè¿›è¡Œå¾®è°ƒ

```
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",num_labels=3)

trainer = Trainer(
    model=model,*# the instantiated ğŸ¤— Transformers model to be trained*
    args=training_args, *# training arguments, defined above*
    train_dataset=train_dataset,*# training dataset*
    eval_dataset=val_dataset , *# evaluation dataset*
    compute_metrics=compute_metrics,
)

trainer.train()
```

![](img/ec4ca08e4981ee6ae5ddeb338e1d714f.png)

# åŸºäºéªŒè¯æ•°æ®é›†è¯„ä¼°åŸ¹è®­å¸ˆ

å› ä¸ºä½œä¸ºè®­ç»ƒçš„ä¸€éƒ¨åˆ†ï¼Œload_best_model_at_end è¢«è®¾ç½®ä¸º Trueï¼Œæ‰€ä»¥è¿™å°†åœ¨å®Œæˆè®­ç»ƒæ—¶è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ã€‚

```
trainer.evaluate()
```

![](img/dd32e4b33160ebbd791a30f6d305e0ff.png)

# æ ¹æ®æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹

```
test[â€˜Sentimentâ€™] = 0 test_texts = test[â€˜Reviewâ€™].values.tolist() test_labels = test[â€˜Sentimentâ€™].values.tolist() 
test_encodings = tokenizer(test_texts, truncation=**True**, padding=**True**,return_tensors = â€˜ptâ€™).to(â€œcudaâ€) 
test_dataset = SentimentDataset(test_encodings, test_labels)preds = trainer.predict(test_dataset=test_dataset)
```

# æ£€ç´¢é¢„æµ‹æ¦‚ç‡

```
probs = torch.from_numpy(preds[0]).softmax(1)

predictions = probs.numpy()# convert tensors to numpy array
```

![](img/b5b5059bfaf504757da724bfcc4f28c0.png)

# å°†ç›¸å…³çš„é¢„æµ‹æ¦‚ç‡è½¬æ¢æˆæ•°æ®å¸§

```
newdf = pd.DataFrame(predictions,columns=['Negative_0','Neutral_1','Positive_2'])
new_df.head()
```

![](img/f39bcdd224c930b56e1a934f26c3e652.png)

# å®šä¹‰å‡½æ•°ä»¥æ ¼å¼åŒ–é¢„æµ‹æ ‡ç­¾

```
**def** labels(x):
  **if** x == 0:
    **return** 'Negative_0'
  **elif** x == 1:
    **return** 'Neutral_1'
  **else**:
    **return** 'Positive_2'

results = np.argmax(predictions,axis=1)
test['Sentiment'] = results
test['Sentiment'] = test['Sentiment'].map(labels)
test.head()
```

![](img/b92fe0979801d8754e6ee352df42382a.png)

# æƒ³è±¡ä¸€ä¸‹é¢„æµ‹

```
**import** **seaborn** **as** **sns**
sns.countplot(x='Sentiment',data=test)
```

![](img/ae1c89719f8204887a85e357872e75a3.png)

# ç»“è®º

*   åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šä½¿ç”¨ huggingface transformers åº“è®­ç»ƒäº† BERT æ¨¡å‹
*   åŒæ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–å‹å·çš„å˜å‹å™¨ï¼Œå¦‚å¸¦æœ‰ GPT 2 çš„ GPT-2 å˜å‹å™¨ã€å¸¦æœ‰ distilt for sequence classification çš„ DistilBert å˜å‹å™¨ç­‰ã€‚

# æ¨èäºº:

[](https://huggingface.co/transformers/training.html) [## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

### PyTorch å’Œ TensorFlow 2.0 çš„æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‚å˜å½¢é‡‘åˆšæä¾›äº†æˆåƒä¸Šä¸‡çš„â€¦

huggingface.co](https://huggingface.co/transformers/training.html) 

## [æ•°æ®](https://machinehack.com/practice/sentiment_analysis_with_rnn/overview)

[è”ç³»æˆ‘](https://www.linkedin.com/in/plaban-nayak-a9433a25/)