# å»ºç«‹è¯†åˆ«å‡æ–°é—»æ–‡ç« çš„ç³»ç»Ÿï¼

> åŸæ–‡ï¼š<https://medium.com/nerd-for-tech/build-a-system-to-identify-fake-news-articles-6604968043cb?source=collection_archive---------10----------------------->

## ä½¿ç”¨æ–‡æœ¬åˆ†æå’Œç»å…¸çš„æœºå™¨å­¦ä¹ æ–¹æ³•è§£å†³ç°å®ä¸–ç•Œçš„é—®é¢˜ï¼

![](img/70b1d40cabf20d41212b1ff1a1feca84.png)

ä¸æŠ¥çº¸ã€æ‚å¿—ã€å¹¿æ’­å’Œç”µè§†ç­‰ä¼ ç»Ÿå¤§ä¼—åª’ä½“ç›¸æ¯”ï¼Œå‡æ–°é—»çš„æ™®éä¼ æ’­æ˜¯ç¤¾äº¤ç½‘ç»œä¼ æ’­æ–°é—»æ‰©å¼ çš„å‰¯ä½œç”¨ã€‚äººç±»åˆ†è¾¨çœŸå‡äº‹å®çš„æ•ˆç‡ä½ä¸‹æš´éœ²äº†å‡æ–°é—»å¯¹é€»è¾‘çœŸç†ã€æ°‘ä¸»ã€æ–°é—»å’Œæ”¿åºœæœºæ„å…¬ä¿¡åŠ›çš„å¨èƒã€‚

ä¿¡æ¯çš„çœŸå®æ€§æ˜¯å…¶å®Œæ•´æ€§çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å¯¹å‡æ–°é—»çš„æ‰“å‡»ä½¿å¾—åº”ç”¨å±‚å¯¹ç¤¾äº¤ç½‘ç»œä¿¡æ¯å’Œæ•°æ®æ¶ˆè´¹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§æ£€æŸ¥å˜å¾—ä¸å¯åˆ†å‰²ã€‚è™šå‡å†…å®¹çš„æŠ«éœ²æ„å‘³ç€å¤„ç†å’Œç½‘ç»œèµ„æºçš„æµªè´¹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯¹æ‰€æä¾›æœåŠ¡çš„ä¿¡æ¯å®Œæ•´æ€§å’Œå¯ä¿¡åº¦æ„æˆäº†ä¸¥é‡å¨èƒã€‚å› æ­¤ï¼Œä¸çœŸå®ä¿¡æ¯çš„åˆ†äº«å…³ç³»åˆ°åº”ç”¨äºæ–°é—»ä¼ æ’­çš„**ä¿¡ä»»è´¨é‡(QoT)** ï¼ŒæŒ‡çš„æ˜¯ç”¨æˆ·å¯¹ç‰¹å®šæ¥æºçš„å†…å®¹çš„ä¿¡ä»»ç¨‹åº¦ã€‚

![](img/4b5765a1b56d93e3568b9d5168dca976.png)

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ç”¨è‡ªç„¶è¯­è¨€é¢„å¤„ç†æ•°æ®ã€çŸ¢é‡åŒ–ã€é™ç»´ã€æœºå™¨å­¦ä¹ å’Œä¿¡æ¯æ£€ç´¢è´¨é‡è¯„ä¼°çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä¹Ÿå°†å‡æ–°é—»çš„è¯†åˆ«ç½®äºè¯­å¢ƒä¸­ã€‚

## å•†ä¸šé—®é¢˜

å¼€å‘ä¸€ä¸ªæœºå™¨å­¦ä¹ ç¨‹åºæ¥è¯†åˆ«ä¸€ç¯‡æ–‡ç« ä½•æ—¶å¯èƒ½æ˜¯å‡æ–°é—»ã€‚

## ç”¨æˆ·æŒ‡å—

[**Colab ç¬”è®°æœ¬**](https://github.com/Priyanka-Dandale/Fake-News-Kaggle-Competition) å¯ä»¥åœ¨æˆ‘çš„ [GitHub](https://github.com/Priyanka-Dandale/Fake-News-Kaggle-Competition) ä»“åº“ä¸­è·å¾—è¿™ä¸ªçœŸå®ä¸–ç•Œçš„ç”¨ä¾‹ï¼

## ç†è§£æ•°æ®

æ‚¨å¯ä»¥ä»[***Kaggle***](https://www.kaggle.com/c/fake-news/data)***ä¸‹è½½ç”¨ä¾‹çš„æ•°æ®é›†ã€‚***

***ä»€ä¹ˆæ˜¯å‡æ–°é—»ï¼Ÿâ€”*** å‡æ–°é—»ä¸€è¯æœ€åˆæŒ‡çš„æ˜¯åœ¨ç›¸å…³æ–°é—»çš„å¹Œå­ä¸‹ä¼ æ’­çš„è™šå‡ä¸”ç»å¸¸æ˜¯è€¸äººå¬é—»çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¯çš„ç”¨æ³•å·²ç»å‘ç”Ÿäº†å˜åŒ–ï¼Œç°åœ¨è¢«è®¤ä¸ºæ˜¯ç¤¾äº¤åª’ä½“ä¸Šä¼ æ’­è™šå‡ä¿¡æ¯çš„åŒä¹‰è¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®è°·æ­Œè¶‹åŠ¿ï¼Œâ€œå‡æ–°é—»â€ä¸€è¯åœ¨ 2017 å¹´éå¸¸æµè¡Œã€‚

> å‡æ–°é—»è¢«å®šä¹‰ä¸ºæ•…æ„å’Œæ˜æ˜¾è™šå‡çš„æ–°é—»ï¼Œæˆ–ä»»ä½•å‘ˆç°ä¸ºäº‹å®ä¸Šä¸æ­£ç¡®çš„æ–°é—»çš„ä¿¡æ¯ï¼Œæ—¨åœ¨è¯¯å¯¼æ–°é—»æ¶ˆè´¹è€…ç›¸ä¿¡å®ƒæ˜¯çœŸå®çš„ã€‚

## **æ•°æ®æ¦‚è¿°**

æ•°æ®å°†åœ¨ä¸€ä¸ªæ–‡ä»¶*ä¸­æ€»å…±æœ‰ 20800 ä¸ª*ä¸ªè§‚æµ‹å€¼æˆ–è¡Œæ•°ã€‚å®ƒåŒ…å« ***5* åˆ—** : id(æ–°é—»æ–‡ç« çš„å”¯ä¸€ id)ã€title(æ–°é—»æ–‡ç« çš„æ ‡é¢˜)ã€author(æ–°é—»æ–‡ç« çš„ä½œè€…)ã€text(æ–‡ç« çš„æ­£æ–‡ï¼›å¯èƒ½æ˜¯ä¸å®Œæ•´çš„)ã€æ ‡ç­¾(å°†ç‰©å“æ ‡è®°ä¸ºæ½œåœ¨ä¸å¯é çš„æ ‡ç­¾)ï¼Œç”¨ 1(ä¸å¯é æˆ–ä¼ªé€ )æˆ– 0(å¯é )è¡¨ç¤ºã€‚**

**![](img/9576b55420c2bca8e62e862cd65933e0.png)**

**å‰ 10 ä¸ªæ•°æ®è§‚å¯Ÿ**

```
**import pandas as pd
df = pd.read_csv(r"/content/train.csv",error_bad_lines=False)# error_bad_lines=False -- cause the offending lines to be skipped.print(df.shape)# first 10 lines of the dataprint(df.head(10))**
```

## **ML é—®é¢˜çš„ç±»å‹**

**è¿™æ˜¯ä¸€ä¸ª ***äºŒå…ƒåˆ†ç±»é—®é¢˜*** ï¼Œå¯¹äºä¸€ç¯‡ç»™å®šçš„æ–°é—»æ–‡ç« ï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹å®ƒæ˜¯å¦å¯é ã€‚**

## **åŸºæœ¬æ¢ç´¢æ€§æ•°æ®åˆ†æ**

*****è®­ç»ƒçš„è§‚æµ‹å€¼æ€»æ•°æ˜¯å¤šå°‘ï¼Ÿ****â€”â€”*20800*ã€‚***

*****æ•°æ®ä¸­æ˜¯å¦æœ‰ç¼ºå¤±å€¼ï¼Ÿ*****

**![](img/674c7d689e23f13185b7a2454292dc14.png)**

*   **ç¼ºå¤±å€¼çš„ç™¾åˆ†æ¯”æ›´å°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä¸¢å¼ƒé‚£äº›è§‚å¯Ÿå€¼ã€‚**

```
**# Count of missing valuesprint(pd.DataFrame(df.isnull().sum(),columns=['Count of missing values']))# Percentage of missing valuesprint(pd.DataFrame(round(df.isnull().sum()*100/df.shape[0],2),columns=['Percentage of missing values']))# Drop missing observations
df.dropna(inplace=True)
print(df.shape)#resetting the index after dropping missing rows
df.reset_index(inplace=False,drop=True)df.head(10)**
```

*****é˜¶çº§æ ‡ç­¾çš„åˆ†å¸ƒæ˜¯æ€æ ·çš„ï¼Ÿ*****

****ä¸å¯é æ–‡ç« (å‡æˆ– 1)** çš„æ•°é‡æ˜¯ 7924 ç¯‡ï¼Œ**å¯é æ–‡ç« (0)** çš„æ•°é‡æ˜¯ 10361 ç¯‡ï¼Œæ•°æ®ä¸ç¬¦åˆä¸å¹³è¡¡æƒ…å†µï¼Œä½†å‡ ä¹ 43%çš„æ–‡ç« æ˜¯å‡çš„ï¼ğŸ˜¥**

**![](img/40dd874e7c3dfe4adab49322a4dcab8f.png)**

```
**import seaborn as sns
sns.countplot(x = df['label']);print("1 : Unreliable or fake")
print("0 : Reliable")
print("\nDistribution of labels is:")
print(df.label.value_counts());print(round(df.label.value_counts(normalize=True),2)*100);**
```

*****å‡æ–°é—»æ–‡ç« çš„å¸¸è§ä½œè€…æœ‰å“ªäº›ï¼Ÿ*****

**![](img/833a0c87060094f0f2be0be5130f27a1.png)**

```
**from wordcloud import WordCloud, STOPWORDS
from os import path
import matplotlib.pyplot as pltdf_unreliable = df[df['label'] == 1]
df_reliable = df[df['label'] == 0]textp_w = df_unreliable['author']
stopwords = set(STOPWORDS)print ("Word Cloud for Authors of Fake news articles:")
wordcloud = WordCloud().generate(' '.join(textp_w))
plt.figure(figsize=(10,10))# Generate plot
plt.imshow(wordcloud)
plt.axis("off")
plt.show()**
```

*****å“ªäº›æ˜¯å¯é æ–°é—»æ–‡ç« çš„å¸¸è§ä½œè€…ï¼Ÿ*****

**![](img/5d1ba9b40de14976c00fd43f4fd3be60.png)**

```
**textp_w = df_reliable['author']
stopwords = set(STOPWORDS)print ("Word Cloud for Authors of Reliable news articles:")
wordcloud = WordCloud().generate(' '.join(textp_w))
plt.figure(figsize=(10,10))# Generate plot
plt.imshow(wordcloud)
plt.axis("off")
plt.show()**
```

## **æ•°æ®é¢„å¤„ç†**

*****ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬æ•°æ®è½¬æ¢æˆæ•°å€¼æ•°æ®ï¼Ÿ*****

*   **è®¡ç®—æœºåªç†è§£æ•°å­—ã€‚**
*   **ä¸€æ—¦æˆ‘ä»¬å°†æ–‡æœ¬è½¬æ¢æˆå‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨çº¿æ€§ä»£æ•°çš„ä¼˜ç‚¹ã€‚**

**åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹æœ¬å¸–ä¸­ä¼šç»å¸¸ç”¨åˆ°çš„ä¸¤ä¸ªæœ¯è¯­ã€‚**

*   ****æ–‡æ¡£** -å®ƒåªæ˜¯ä¸€ä¸ªåŒ…å«æ–‡æœ¬æ•°æ®çš„æ–‡ä»¶ã€‚å°±æ•°æ®é›†è€Œè¨€ï¼Œæ¯ä¸ªè®°å½•æˆ–æ•°æ®ç‚¹éƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªæ–‡æ¡£ã€‚**
*   ****æ–‡é›†** -ä¸€ç»„æ–‡æ¡£è¢«ç§°ä¸ºä¸€ä¸ªæ–‡é›†ã€‚å°±æ•°æ®é›†è€Œè¨€ï¼Œæ•´ä¸ªæ•°æ®ç‚¹æˆ–æ•´ä¸ªæ•°æ®é›†å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªè¯­æ–™åº“ã€‚**

**ç°åœ¨æˆ‘ä»¬å¯ä»¥èµ°äº†ï¼ï¼**

**æ–‡æœ¬é¢„å¤„ç†çš„ç¬¬ä¸€æ­¥æ˜¯ä»â€œæ ‡é¢˜â€ç‰¹å¾ä¸­å»é™¤ç‰¹æ®Šå­—ç¬¦ã€è¯å¹²å’Œåœç”¨è¯ã€‚å¦‚æœä½ æƒ³äº†è§£å¦‚ä½•åœ¨æ–‡æœ¬é¢„å¤„ç†ä¸Šæ‰§è¡Œä¸Šè¿°æ­¥éª¤ï¼Œé‚£ä¹ˆè¯·æŸ¥é˜… [**Colab ç¬”è®°æœ¬**](https://github.com/Priyanka-Dandale/Fake-News-Kaggle-Competition) **ã€‚æˆ‘å·²ç»ç”¨ä¸€ä¸ªä¾‹å­è§£é‡Šäº†è¿™äº›æ­¥éª¤ï¼****

> **æ³¨æ„:å¯¹äºé¢„å¤„ç†å’Œæ¨¡å‹æ„å»ºï¼Œæˆ‘åªè€ƒè™‘äº†â€œæ ‡é¢˜â€å’Œâ€œæ ‡ç­¾â€ç‰¹æ€§ã€‚æ‚¨è¿˜å¯ä»¥åŒ…æ‹¬å…¶ä»–åŠŸèƒ½ã€‚æˆ‘å°è¯•è¿‡ä¿ç•™â€œæ–‡æœ¬â€,ä½†æ˜¯æ€§èƒ½æŒ‡æ ‡æ˜¯ä¸å¯é çš„ï¼**

```
**import nltk
nltk.download('stopwords')
import re
from nltk.corpus import stopwordsSTOP_WORDS = stopwords.words("english")
from tqdm import tqdmdef preprocess(documents):
    corpus=[]
    sentences=[]
    for i in tqdm(range(0,len(documents))):
         news = re.sub('[^a-zA-Z]',' ', documents['title'][i])
         news = news.lower()
         tokens = news.split()
         words = [PorterStemmer().stem(token) for token in tokens if    
         token not in set(stopwords.words('english'))] sentences =' '.join(words)
         corpus.append(sentences)
   return corpus
corpus = preprocess(df)**
```

**æ‰§è¡Œä¸Šè¿°é¢„å¤„ç†æ­¥éª¤åï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è½¬æ¢ä¸ºæ•°å­—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç®€å•çš„[****ã€å¼“å½¢ã€‘****](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) **çš„æ–¹æ³•ã€‚****

## ****åŸºäºè¯åŒ…çš„ç‰¹å¾æå–****

****å•è¯åŒ…æ˜¯æè¿°å•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ–‡æœ¬è¡¨ç¤ºã€‚å®ƒæ¶‰åŠä¸¤ä»¶äº‹:****

*   ****å·²çŸ¥å•è¯çš„è¯æ±‡è¡¨ã€‚****
*   ****å·²çŸ¥å•è¯å­˜åœ¨çš„ä¸€ç§åº¦é‡ã€‚****

****å®ƒè¢«ç§°ä¸ºå•è¯çš„***åŒ…*ï¼Œå› ä¸ºä»»ä½•å…³äºæ–‡æ¡£ä¸­å•è¯çš„é¡ºåºæˆ–ç»“æ„çš„ä¿¡æ¯éƒ½è¢«ä¸¢å¼ƒäº†ã€‚è¯¥æ¨¡å‹åªå…³å¿ƒå·²çŸ¥å•è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æ¡£ä¸­ï¼Œè€Œä¸å…³å¿ƒå®ƒåœ¨æ–‡æ¡£ä¸­çš„ä½ç½®ã€‚******

> ****å¥å­å’Œæ–‡æ¡£çš„ä¸€ä¸ªéå¸¸å¸¸è§çš„ç‰¹å¾æå–è¿‡ç¨‹æ˜¯å•è¯è¢‹æ–¹æ³•ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æŸ¥çœ‹æ–‡æœ¬ä¸­å•è¯çš„ç›´æ–¹å›¾ï¼Œå³å°†æ¯ä¸ªå•è¯è®¡æ•°è§†ä¸ºä¸€ä¸ªç‰¹å¾ã€‚****

****è®©æˆ‘ä»¬ä»ä¸‹é¢çš„ä¾‹å­ä¸­äº†è§£ä¸€ä¸‹:****

****![](img/cf772e42877e101fdc78a593de20f576.png)****

****è¿™é‡Œï¼Œcatã€satã€inã€hatã€with æ˜¯ 3 ä¸ªæ–‡æ¡£çš„ 6 ä¸ªç‰¹å¾ï¼Œæ¡ç›®æ˜¯è¿™äº›ç‰¹å¾çš„é¢‘ç‡ã€‚å¾ˆç®€å•ã€‚ä¸æ˜¯å—ï¼Ÿ****

```
****from sklearn.feature_extraction.text import CountVectorizercv = CountVectorizer(max_features=5000,ngram_range=(1,3))
X = cv.fit_transform(corpus).toarray()print(X.shape)
print(cv.get_featurbe_names()[0:7])****
```

****æˆ‘ä»¬å°†ä»å¼“ä¸Šè·å¾— 5000 ä¸ªç‰¹å¾ã€‚****

****![](img/7c57dcd9cdca327f03ebcee547f21a88.png)****

****7 å¤§åŠŸèƒ½åç§°****

## ******åˆ—è½¦æµ‹è¯•åˆ†å‰²******

****è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»¥ 75:25 çš„æ¯”ä¾‹å°†æˆ‘ä»¬çš„æ•°æ®åˆ†æˆè®­ç»ƒéªŒè¯(æµ‹è¯•)ã€‚åˆ—è½¦æ•°æ®ä¸­çš„æ•°æ®ç‚¹æ•°: *21141* å’ŒéªŒè¯æ•°æ®ä¸­çš„æ•°æ®ç‚¹æ•°: *7048* ã€‚****

****åœ¨å°†æ•°æ®åˆ†æˆè®­ç»ƒå’ŒéªŒè¯ä¹‹åï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸‹é¢çš„ç±»æ ‡ç­¾åˆ†å¸ƒï¼Œè¿™è¡¨æ˜æ•°æ®ä¸éµå¾ªä¸å¹³è¡¡æ ‡å‡†ã€‚****

****![](img/12c197e7811476832a617c628bb352b4.png)****

```
****y_true = df['label']from sklearn.model_selection import train_test_splitX_train,X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.25, random_state=42)print("Number of data points in train data :",X_train.shape)
print("Number of data points in validation data :",X_test.shape)from collections import Counter
print("-"*10, "Distribution of output variable in train data", "-"*10)train_distr = Counter(y_train)
train_len = len(y_train)print("Class 0: ",round(int(train_distr[0])*100/train_len,2),"Class 1: ", round(int(train_distr[1])*100/train_len,2))print("-"*10, "Distribution of output variable in validation data", "-"*10)test_distr = Counter(y_test)
test_len = len(y_test)print("Class 0: ",round(int(test_distr[0])*100/test_len,2), "Class 1: ",round(int(test_distr[1])*100/test_len,2))****
```

## ****éšæœºåŸºçº¿æ¨¡å‹****

****åŸºçº¿é¢„æµ‹ç®—æ³•æä¾›äº†ä¸€ç»„é¢„æµ‹ï¼Œæ‚¨å¯ä»¥åƒå¯¹é—®é¢˜çš„ä»»ä½•é¢„æµ‹ä¸€æ ·å¯¹è¿™äº›é¢„æµ‹è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚åˆ†ç±»å‡†ç¡®æ€§æˆ–æŸå¤±ã€‚éšæœºé¢„æµ‹ç®—æ³•é¢„æµ‹åœ¨è®­ç»ƒæ•°æ®ä¸­è§‚å¯Ÿåˆ°çš„éšæœºç»“æœã€‚è¿™æ„å‘³ç€éšæœºæ¨¡å‹éšæœºé¢„æµ‹æ ‡ç­¾ 0 æˆ– 1ã€‚å½“è¯„ä¼°æ‰€æœ‰å…¶ä»–æœºå™¨å­¦ä¹ ç®—æ³•æ—¶ï¼Œè¿™äº›ç®—æ³•çš„åˆ†æ•°æä¾›äº†æ‰€éœ€çš„æ¯”è¾ƒç‚¹â€¦****

> ******ä½¿ç”¨éšæœºæ¨¡å‹çš„éªŒè¯æ•°æ®çš„å‡†ç¡®åº¦ä¸º 50%ã€‚**å¤§é‡æé«˜ç²¾åº¦çš„ç¤ºæ³¢å™¨ï¼****

****![](img/fd2e3ed2f4ebbac65548108f5ee200df.png)********![](img/d3d04cbf006fe9ea127a15c1f2f64bb7.png)****

```
****from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics.classification import accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns# we create a output array that has exactly same size as the CV datapredicted_y = np.zeros((test_len,2))
for i in range(test_len):
    rand_probs = np.random.rand(1,2)
    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])predicted_y =np.argmax(predicted_y, axis=1)print("Accuracy on Validation Data using Random Model",round(accuracy_score(y_test, predicted_y),2))
plot_confusion_matrix(y_test, predicted_y)****
```

## ****å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨****

****å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç®—æ³•æ˜¯ä¸€ç§æ¦‚ç‡å­¦ä¹ æ–¹æ³•ï¼Œä¸»è¦ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€‚è¯¥ç®—æ³•åŸºäºè´å¶æ–¯å®šç†ï¼Œå¹¶é¢„æµ‹æ–‡æœ¬(å¦‚ä¸€å°ç”µå­é‚®ä»¶æˆ–ä¸€ç¯‡æŠ¥çº¸æ–‡ç« )çš„æ ‡ç­¾ã€‚å®ƒè®¡ç®—ç»™å®šæ ·æœ¬ä¸­æ¯ä¸ªæ ‡ç­¾çš„æ¦‚ç‡ï¼Œç„¶åå°†æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾ä½œä¸ºè¾“å‡ºã€‚****

****æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯è®¸å¤šç®—æ³•çš„é›†åˆï¼Œå…¶ä¸­æ‰€æœ‰ç®—æ³•å…±äº«ä¸€ä¸ªå…±åŒçš„åŸåˆ™ï¼Œå³è¢«åˆ†ç±»çš„æ¯ä¸ªç‰¹å¾ä¸ä»»ä½•å…¶ä»–ç‰¹å¾éƒ½ä¸ç›¸å…³ã€‚ä¸€ä¸ªç‰¹å¾çš„å­˜åœ¨ä¸å¦ä¸å½±å“å¦ä¸€ä¸ªç‰¹å¾çš„å­˜åœ¨ä¸å¦ã€‚****

> ******ä½¿ç”¨ MNB æ¨¡å‹å¯¹åˆ—è½¦æ•°æ®çš„å‡†ç¡®ç‡ä¸º 92%******
> 
> ******ä½¿ç”¨ MNB æ¨¡å‹éªŒè¯æ•°æ®çš„å‡†ç¡®ç‡ä¸º 90%******

****![](img/1f11c78c966e7e0b19623b7cabd767ce.png)********![](img/1752b03445de5609fff0ef22642c382b.png)****

```
****from sklearn.naive_bayes import MultinomialNBclassifier=MultinomialNB()
classifier.fit(X_train,y_train)predicted_y_train = classifier.predict_proba(X_train)
predicted_y_train = np.argmax(predicted_y_train,axis=1)print("Accuracy on Train Data using NB Model",round(metrics.accuracy_score(y_train, predicted_y_train),2))predict_y = classifier.predict_proba(X_test)
predicted_y =np.argmax(predict_y,axis=1)#print("Total number of validation data points :", len(predicted_y))print("Accuracy on Validation Data using NB Model",round(metrics.accuracy_score(y_test,predicted_y),2))# Go to Colab notebook for plot_confusion_matrix function
plot_confusion_matrix(y_test, predicted_y)****
```

## ****è¢«åŠ¨ä¸»åŠ¨åˆ†ç±»å™¨****

****è¢«åŠ¨æ”»å‡»ç®—æ³•ä¸€èˆ¬ç”¨äºå¤§è§„æ¨¡å­¦ä¹ ã€‚è¿™æ˜¯å°‘æ•°åœ¨çº¿å­¦ä¹ ç®—æ³•ä¹‹ä¸€ã€‚åœ¨åœ¨çº¿ ML ç®—æ³•ä¸­ï¼Œè¾“å…¥æ•°æ®æŒ‰é¡ºåºå‡ºç°ï¼ŒML æ¨¡å‹é€æ­¥æ›´æ–°ã€‚è¿™åœ¨å­˜åœ¨å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ï¼Œå¹¶ä¸”ç”±äºæ•°æ®çš„åºå¤§ï¼Œè®­ç»ƒæ•°æ®åœ¨è®¡ç®—ä¸Šæ˜¯ä¸å¯è¡Œçš„ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´ï¼Œåœ¨çº¿å­¦ä¹ ç®—æ³•å°†è·å¾—ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ›´æ–°åˆ†ç±»å™¨ï¼Œç„¶åä¸¢å¼ƒè¯¥æ ·æœ¬ã€‚ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯æ£€æµ‹ Twitter ä¸Šçš„å‡æ–°é—»ï¼Œé‚£é‡Œæ¯ç§’é’Ÿéƒ½æœ‰æ–°æ•°æ®è¢«æ·»åŠ è¿›æ¥ã€‚****

*******è¿™ä¸ªç®—æ³•æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ*******

****ç®€å•æ¥è¯´:****

*   ******è¢«åŠ¨:**ä¸ºäº†æ­£ç¡®é¢„æµ‹ï¼Œä¿æŒæ¨¡å‹ï¼Œä¸åšä»»ä½•æ”¹å˜ã€‚****
*   ******ç§¯æçš„:**å¯¹äºä¸æ­£ç¡®çš„é¢„æµ‹ï¼Œå¯¹æ¨¡å‹åšå‡ºæ”¹å˜å¯èƒ½æ˜¯å¯¹æ¨¡å‹çš„æŸç§æ”¹å˜ï¼Œå¯ä»¥çº æ­£å®ƒã€‚****

****è¦äº†è§£æ›´å¤šå…³äºè¿™ç§ç®—æ³•èƒŒåçš„æ•°å­¦çŸ¥è¯†ï¼Œæˆ‘æ¨èè§‚çœ‹ç”± Victor Lavrenko åšå£«åˆ¶ä½œçš„å…³äºè¿™ç§ç®—æ³•å·¥ä½œçš„ç²¾å½©è§†é¢‘ã€‚****

> ******ä½¿ç”¨è¢«åŠ¨å›å½’æ¨¡å‹å¯¹åˆ—è½¦æ•°æ®çš„å‡†ç¡®ç‡ä¸º 100%******
> 
> ******è¢«åŠ¨å›å½’æ¨¡å‹éªŒè¯æ•°æ®çš„å‡†ç¡®ç‡ä¸º 96%******

****![](img/2f27617b46329862218fbe5cef660cf4.png)********![](img/c80bf872122b5129e6594ee19a73901e.png)****

```
****from sklearn.linear_model import PassiveAggressiveClassifierlinear_clf=PassiveAggressiveClassifier()# Fitting model
linear_clf.fit(X_train, y_train)predictions=linear_clf.predict(X_test)
predicted_y_train = linear_clf.predict(X_train)print("Accuracy on Train Data using PassiveAgressive Model",round(metrics.accuracy_score(y_train, predicted_y_train),2))pred = linear_clf.predict(X_test)print("Total number of validation data points :", len(predicted_y))
print("Accuracy on Validation Data using PassiveAgressive Model",round(metrics.accuracy_score(y_test,pred),2))plot_confusion_matrix(y_test, pred)****
```

****æ„Ÿè°¢ä½ é˜…è¯»â¤****

## ****å‚è€ƒ****

*   ******æ–‡å­—è¢‹æ¨¡å‹**ç®€ä»‹ä½œè€…â€” [**æ°æ£®Â·å¸ƒæœ—åˆ©**](https://machinelearningmastery.com/author/jasonb/)****

## ****èµ„æº****

*   ******Kaggle ç«èµ›â€”**[**https://www.kaggle.com/c/fake-news/data**](https://www.kaggle.com/c/fake-news/data)**ã€‚******

****__________________________________________________________________****

****å¯¹äºä»»ä½•å»ºè®®æˆ–ç–‘é—®ï¼Œè¯·åœ¨ä¸‹é¢ç•™ä¸‹æ‚¨çš„è¯„è®ºï¼Œå¹¶å…³æ³¨æ›´æ–°ã€‚****

****å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·ç‚¹å‡»ğŸ‘å›¾æ ‡æ¥æ”¯æŒå®ƒã€‚è¿™å°†æœ‰åŠ©äºå…¶ä»–åª’ä½“ç”¨æˆ·æ‰¾åˆ°å®ƒã€‚åˆ†äº«ä¸€ä¸‹ï¼Œè®©åˆ«äººä¹Ÿèƒ½çœ‹ï¼****

****å¿«ä¹å­¦ä¹ ï¼ğŸ˜Š****