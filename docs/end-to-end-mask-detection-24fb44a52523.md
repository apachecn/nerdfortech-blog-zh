# ç«¯åˆ°ç«¯å±è”½æ£€æµ‹

> åŸæ–‡ï¼š<https://medium.com/nerd-for-tech/end-to-end-mask-detection-24fb44a52523?source=collection_archive---------1----------------------->

![](img/7cf14e5621d093da6b8d4c9f642fdfd2.png)

äºšå½“Â·å°¼è¥¿å¥¥é²å…‹åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

é¦–å…ˆï¼Œä»€ä¹ˆæ˜¯ç«¯åˆ°ç«¯ï¼Ÿ

æœºå™¨å­¦ä¹ /æ•°æ®ç§‘å­¦å®¶ä¸­çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªæ¶µç›–æ•°æ®æ”¶é›†ä»¥æ„å»ºæ¨¡å‹å¹¶åœ¨ç°å®ç”Ÿæ´»ä¸­ä½¿ç”¨å®ƒçš„è¿‡ç¨‹ã€‚

æ›´å¤šç«¯åˆ°ç«¯é˜…è¯»é˜…è¯»è¿™ä¸ªæƒŠäººçš„åšå®¢

[](https://eugeneyan.com/writing/end-to-end-data-science/) [## ä¸å—æ¬¢è¿çš„è§‚ç‚¹-æ•°æ®ç§‘å­¦å®¶åº”è¯¥æ›´åŠ ç«¯åˆ°ç«¯

### æœ€è¿‘ï¼Œæˆ‘åœ¨ Reddit ä¸Šçœ‹åˆ°ä¸€ä¸ªå…³äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­ä¸åŒè§’è‰²çš„å¸–å­:æ•°æ®ç§‘å­¦å®¶â€¦

eugeneyan.com](https://eugeneyan.com/writing/end-to-end-data-science/) 

ç›®å½•

1.  æ”¶é›†æ•°æ®
2.  æ•°æ®é¢„å¤„ç†
3.  å¯è§†åŒ–æ•°æ®
4.  å»ºç«‹æ¨¡å‹
5.  ç«è½¦æ¨¡å‹
6.  è¯„ä¼°è®­ç»ƒæ¨¡å‹
7.  ä¿å­˜å·²è®­ç»ƒçš„æ¨¡å‹
8.  æ ¹æ®è‡ªå®šä¹‰æ•°æ®è¿›è¡Œé¢„æµ‹
9.  å®æ—¶æ£€æµ‹

# 1.æ”¶é›†æ•°æ®

åœ¨æ¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ä¸­ï¼Œé—®é¢˜æ•°æ®éƒ½å¾ˆé‡è¦ã€‚ä½†æ›´é‡è¦çš„æ˜¯å®ƒçš„å½¢å¼æ˜¯æ­£ç¡®çš„ã€‚

è¿™é‡Œï¼Œæ•°æ®æ˜¯ä» [Kaggle](https://www.kaggle.com/andrewmvd/face-mask-detection) æ”¶é›†çš„ã€‚

æ•°æ®æ˜¯è¿™ä¸ªæ•°æ®é›†çš„å­é›†ï¼Œä½ å¯ä»¥ä»[è¿™é‡Œ](https://drive.google.com/drive/folders/1n8AXQ8wEKWH6nx8Vz-dxLbPQzz5qerPq?usp=sharing)ä¸‹è½½

# 2.æ•°æ®é¢„å¤„ç†

åœ¨æˆ‘ä»¬å¤„ç†æ•°æ®ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç”¨æ­£ç¡®çš„åç§°å°†æ•°æ®ç»„ç»‡åœ¨æ­£ç¡®çš„æ–‡ä»¶å¤¹ä¸­ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§é€‰æ‹©:

*   ä½¿ç”¨ tensor flow[image _ dataset _ from _ directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory)ã€‚
*   åŠ è½½ CSV æ–‡ä»¶ã€‚

è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä» CSV æ–‡ä»¶åŠ è½½

åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œä¸åŒ…æ‹¬ CSV æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»é€šè¿‡å›¾åƒæ•°æ®ç”Ÿæˆã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å›¾åƒåç§°æ”¹ä¸ºå¸¦æ©ç å’Œä¸å¸¦æ©ç ã€‚

ç”¨äºå¸¦æ©ç 

```
# importing os module 
import os 

# Function to rename multiple files 
def main():
    for count, filename in enumerate(os.listdir("DATASET/")):
        dst ="withmask." + str(count) + ".jpeg"
        src ='DATASET/'+ filename 
        dst ='DATASET/'+ dst 
        # rename() function will
        # rename all the files 
        os.rename(src, dst) 

# Driver Code 
if __name__ == '__main__':
    # Calling main() function 
    main()
```

å› ä¸ºæ²¡æœ‰é¢å…·

```
# importing os module 
import os 

# Function to rename multiple files 
def main():
    for count, filename in enumerate(os.listdir("New folder/")):
        dst ="withoutmask." + str(count) + ".jpeg"
        src ='New folder/'+ filename 
        dst ='New folder/'+ dst 
        # rename() function will
        # rename all the files 
        os.rename(src, dst) 

# Driver Code 
if __name__ == '__main__':
    # Calling main() function 
    main()
```

ç°åœ¨å°†æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒç§»åŠ¨åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªç†ŠçŒ«æ•°æ®æ¡†

```
import pandas as pd
filenames=os.listdir("FULL_DATA/")
categories=[]
for f_name in filenames:
    category=f_name.split('.')[0]
    if category=='withmask':
        categories.append('withmask')
    else:
        categories.append('withoutmask')
df=pd.DataFrame({
    'filename':filenames,
    'labels':categories
})
```

![](img/c5df791ad66c095e45111da4d7a18849.png)

æ•°æ®å¸§

å°†æ•°æ®å¸§ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚

**ç°åœ¨æ•°æ®ç»“æ„æ­£ç¡®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½æ•°æ®äº†**

*   è¯»å– CSV æ–‡ä»¶

```
# Import pandasimport pandas as pdlabels_csv = pd.read_csv("PATH TO CSV FILE")
```

*   å‡†å¤‡æ ‡ç­¾

```
import numpy as nplabels = labels_csv["labels"].to_numpy()labels
```

*   å°†æ ‡ç­¾è½¬æ¢æˆå¸ƒå°”æ•°ç»„

```
# Find the unique label valuesunique_category = np.unique(labels)len(unique_category)boolean_labels = [label == unique_category for label in labels]
```

*   ä½¿ç”¨ **train_test_split** åˆ›å»ºéªŒè¯é›†

```
# Setup X & y variablesX = filenamesy = boolean_labels# Let's split datafrom sklearn.model_selection import train_test_split# Into train and validX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2509)len(X_train), len(X_val), len(y_train), len(y_val)
```

*   å°†å›¾åƒè½¬æ¢æˆå¼ é‡

```
# Define image size
IMG_SIZE = 224

# Function
def process_image(image_path, image_size=IMG_SIZE):
  """
  Takes an image file path and turns the image into a Tensor.
  """
  # Read in an image file
  image = tf.io.read_file(image_path)
  # Turn the jpg image into numerical Tensor with 3 colour     #channel(RGB)

  image = tf.image.decode_jpeg(image,channels=3)
  # Convert the color channel values to (0-1) values
  image = tf.image.convert_image_dtype(image,tf.float32)
  # Resize the image to (224,224)
  image = tf.image.resize(image, size=[image_size,image_size])

  return image
```

*   å°†æ•°æ®æ‰¹é‡åŒ–

```
# Create a function to return a tuple (image, label)
def get_image_lable(image_path,label):
  """
  Takes an image file path name and the label,
  processes the image and return a tuple (image, label).
  """
  image = process_image(image_path)

  return image, label# Define the batch size
BATCH_SIZE = 32

# Function to convert data into batches
def create_data_batches(X,y=None, batch_size=BATCH_SIZE,valid_data=False):
  """
  Creates batches of data of image (X) and label (y) pairs.
  Shuffle the data if it's training data but doesn't shuffle if it's validation data.
  """
  # If data is valid dataset (NO SHUFFLE)
  if valid_data:
    print("Creating valid data batches.........")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                               tf.constant(y)))
    data_batch = data.map(get_image_lable).batch(batch_size)
    return data_batch

  else:
    print("Creating train data batches.........")
    # Turn filepaths and labels into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                               tf.constant(y)))
    # Shuffling pathname and labels before mapping image processor fun
    data = data.shuffle(buffer_size=len(X))
    data_batch = data.map(get_image_lable).batch(batch_size)

    return data_batch
```

# 3.å¯è§†åŒ–æ•°æ®

```
import matplotlib.pyplot as plt
# Create fun for viewing in a data batch
def show_images(images, labels):
  """
  Displays a plot of 25 images and their labels from a data batch.
  """
  plt.figure(figsize=(20, 20))
  for i in range(25):
    # Subplot
    ax = plt.subplot(5,5,i+1)
    plt.imshow(images[i])
    plt.title(unique_category[labels[i].argmax()])
    plt.axis("Off")
```

è°ƒç”¨è¿™ä¸ªå‡½æ•°

*   å¯¹äºè®­ç»ƒæ•°æ®

```
train_images, train_labels = next(train_data.as_numpy_iterator())
show_images(train_images,train_labels)
```

*   å¯¹äºæœ‰æ•ˆæ•°æ®

```
val_images, val_labels = next(val_data.as_numpy_iterator())
show_images(val_images, val_labels)
```

# 4.å»ºç«‹æ¨¡å‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å°† TensorFlow hub ç”¨äºé¢„è®­ç»ƒæ¨¡å‹ã€‚

å¯¹äºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨å°å‹å‹å·çš„ [MobileNet V2](https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4) ã€‚

1.  Set input_shape = [noneï¼Œ224ï¼Œ224ï¼Œ3]
2.  è®¾ç½®è¾“å‡ºå½¢çŠ¶= 2
3.  ä½¿ç”¨ tf.keras ä¸­çš„é¡ºåºæ¨¡å‹
4.  ä½¿ç”¨ model.build(input_shape)

```
# Create a fun to build a keras model
def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", model_url)

  # Setup the model
  model = tf.keras.Sequential([
                               hub.KerasLayer(model_url),
                               tf.keras.layers.Dense(units=output_shape, 
                                                     activation="softmax")
  ])

  # Compile the model
  model.compile(
      loss = tf.keras.losses.BinaryCrossentropy(),
      optimizer = tf.keras.optimizers.Adam(),
      metrics = ["accuracy"]
  )

  # Build the model
  model.build(input_shape)

  return modelmodel = create_model()
model.summary()
```

![](img/ac6cbae5904fccde829b7f7b6325eb49.png)

æ¨¡å‹ _ æ‘˜è¦

# 5.è®­ç»ƒæ¨¡ç‰¹

æ ¹æ® 25 ä¸ªæ—¶æœŸçš„ train_data å’Œ valid_data è®­ç»ƒæ¨¡å‹

å¦å¤–ï¼Œæ·»åŠ ä¸€ä¸ª[æå‰åœæ­¢å›è°ƒ](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)

```
Model.fit(x=train_data,
          epochs=25,
          validation_data=val_data,
          callback=[early_stopping])
```

![](img/0dc6e4e45a6dc9872e5eb2c0da1c54eb.png)

è®­ç»ƒæ¨¡å‹

æœ‰äº†è¿™ä¸ªæ¨¡å‹ï¼Œval_loss æ˜¯ ***0.0096*** è€Œå‡†ç¡®åº¦å‡ ä¹æ˜¯ ***99.99 %***

# 6.è¯„ä¼°é¢„æµ‹

å¯¹ val_data æ¨¡å‹ä½¿ç”¨ model.predict()è¿”å› shape (_ï¼Œ2)çš„ NumPy æ•°ç»„

![](img/30cdff470e2c5f43512cde35419f62c1.png)

æœ‰æ•ˆæ•°æ®çš„é¢„æµ‹å›¾åƒ

# 7.ä¿å­˜å¹¶é‡æ–°åŠ è½½å·²è®­ç»ƒæ¨¡å‹

ä½¿ç”¨ keras ä¸­çš„ save_model ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚

åŠ è½½æ¨¡å‹ä¸å¸¸è§„çš„ load_model æœ‰ç‚¹ä¸åŒ

è¿™é‡Œæˆ‘ä»¬è¦æä¾›***custom _ objects = { " KerasLayer ":hubã€‚æ¨¡å‹è·¯å¾„æ—åŠ è½½æ¨¡å‹å‡½æ•°ä¸­çš„*** ã€‚

```
model = load_model(
    'model/model.h5', custom_objects={"KerasLayer": hub.KerasLayer})
```

# 8.æ ¹æ®è‡ªå®šä¹‰æ•°æ®è¿›è¡Œé¢„æµ‹

åœ¨é¢„æµ‹æ–°æ•°æ®ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®ƒçš„å½¢çŠ¶å’Œå¤§å°éƒ½æ­£ç¡®ã€‚

```
def test_data(path):
  demo = imread(path)
  demo = tf.image.convert_image_dtype(demo,tf.float32)
  demo = tf.image.resize(demo,size=[224,224])
  demo = np.expand_dims(demo,axis=0)

  pred = model.predict(demo)
  result = unique_category[np.argmax(pred)]

  return result
```

# 9.å®æ—¶æ£€æµ‹

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ OpenCV çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå®æ—¶æ£€æµ‹ã€‚

![](img/45cdb649fc53ee020c431ab154823d40.png)

å¸¦é¢å…·

1.  å¯¼å…¥åº“
2.  ä½¿ç”¨ load_model()åŠ è½½æ¨¡å‹ç¡®ä¿ä½¿ç”¨ ***custom_objectsã€‚***
3.  ä½¿ç”¨ cv2 çš„ç½‘ç»œæ‘„åƒå¤´ã€‚è§†é¢‘æ•è·()
4.  ä½¿ç”¨[Haar scade _ front alface](https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml)XML æ–‡ä»¶è¿›è¡Œäººè„¸æ£€æµ‹ã€‚
5.  ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ä»£ç å›è´­

[](https://github.com/Vivek2509/End-to-end-mask-detector) [## vivek 2509/ç«¯åˆ°ç«¯é¢ç½©æ£€æµ‹å™¨

### ä¸ºé¢å…·æ£€æµ‹æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹ GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œâ€¦

github.com](https://github.com/Vivek2509/End-to-end-mask-detector) 

å˜¿ï¼Œè¯»è€…ä»¬ï¼Œæ„Ÿè°¢ä½ ä»¬çš„æ—¶é—´ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªåšå®¢ï¼Œåˆ«å¿˜äº†é¼“æŒæ¬£èµå®ƒğŸ‘ã€‚

æ›´å¤šå…³äºæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦çš„åšå®¢è¯·å…³æ³¨æˆ‘ã€‚

åŠ¨æ‰‹æœºå™¨å­¦ä¹ é¡¹ç›®å‚è€ƒå¦‚ä¸‹

[](https://vivek2509.github.io/World_of_ML/) [## ML çš„ä¸–ç•Œ

### ä¸ºäº†è®©ä¸–ç•Œå˜å¾—æ›´ç¾å¥½ï¼Œæ˜æ™ºåœ°ä½¿ç”¨æ•°æ®å’Œæœºå™¨ã€‚

vivek2509.github.io](https://vivek2509.github.io/World_of_ML/)